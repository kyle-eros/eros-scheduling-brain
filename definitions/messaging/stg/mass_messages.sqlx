config {
  type: "incremental",
  schema: "eros_messaging_stg",
  uniqueKey: ["message_sk"],
  partitionBy: "sending_date",
  clusterBy: ["username_std"],
  requirePartitionFilter: true,  // Enforced for cost control
  description: "Incremental mass messages with 14-day watermark and robust partition handling",
  labels: {app: "eros", domain: "messaging", layer: "stg"},
  tags: ["messaging_stg"],
  assertions: {
    uniqueKey: ["message_sk"],
    nonNull: ["message_sk", "sending_date", "username_std"]
  }
}

-- Pre-operations: Create empty table structure if it doesn't exist
-- This prevents partition filter errors during initial table creation
pre_operations {
  CREATE TABLE IF NOT EXISTS ${self()} (
    message_sk STRING NOT NULL,
    username_std STRING NOT NULL,
    sending_ts TIMESTAMP,
    sending_date DATE NOT NULL,
    price FLOAT64,
    sent_count INT64,
    viewed_count INT64,
    purchased_count INT64,
    earnings_total FLOAT64,
    message_text STRING,
    source_file STRING,
    loaded_at TIMESTAMP NOT NULL
  )
  PARTITION BY sending_date
  CLUSTER BY username_std
  OPTIONS(
    description="Incremental mass messages staging table with partition enforcement",
    labels=[("app", "eros"), ("layer", "stg")]
  )
}

/*
 * MASS MESSAGES - INCREMENTAL STAGING TABLE
 *
 * Purpose: Consolidates historical and daily mass message data with incremental updates
 *
 * Partitioning Strategy:
 * - Partitioned by sending_date for cost-efficient queries
 * - Requires partition filter to prevent expensive full table scans
 * - Uses 14-day watermark to capture late-arriving data
 *
 * Incremental Logic:
 * - Primary watermark: sending_ts (timestamp precision)
 * - Lookback window: 14 days to catch delayed records
 * - Deduplication: QUALIFY with ROW_NUMBER to ensure unique message_sk
 *
 * Data Quality:
 * - Enforces non-null constraints on key fields
 * - Validates unique message_sk
 * - Monitors for data freshness
 */

WITH
historical_source AS (
  SELECT
    CAST(message_id AS STRING) as message_id,
    CAST(sender AS STRING) as sender,
    CAST(sending_time AS STRING) as sending_time,
    CAST(price AS STRING) as price,
    SAFE_CAST(sent AS INT64) as sent,
    SAFE_CAST(viewed AS INT64) as viewed,
    SAFE_CAST(purchased AS INT64) as purchased,
    CAST(earnings AS STRING) as earnings,
    message as message_text,
    'historical' as source_file
  FROM ${ref("facts_messages_all")}
  WHERE sending_time IS NOT NULL AND TRIM(sending_time) != ''
),

daily_source AS (
  SELECT
    CAST(message_id AS STRING) as message_id,
    CAST(sender AS STRING) as sender,
    CAST(sending_time AS STRING) as sending_time,
    CAST(price AS STRING) as price,
    SAFE_CAST(sent AS INT64) as sent,
    SAFE_CAST(viewed AS INT64) as viewed,
    SAFE_CAST(purchased AS INT64) as purchased,
    CAST(earnings AS STRING) as earnings,
    message as message_text,
    'daily' as source_file
  FROM ${ref("mass_message_daily_final")}
  WHERE sending_time IS NOT NULL AND TRIM(sending_time) != ''
),

messages_unioned AS (
  SELECT * FROM historical_source
  UNION ALL
  SELECT * FROM daily_source
),

messages_cleaned AS (
  SELECT
    ${df_mk_sk(["'MASS_MESSAGES'", "CAST(message_id AS STRING)", "source_file"])} as message_sk,
    ${df_std_username('sender')} as username_std,
    COALESCE(
      SAFE.PARSE_TIMESTAMP('%Y-%m-%dT%H:%M:%E*S%Ez', sending_time),
      SAFE.PARSE_TIMESTAMP('%Y-%m-%d %H:%M:%S', sending_time)
    ) as sending_ts,
    ${df_safe_cast_numeric('price', 'FLOAT64')} as price,
    sent as sent_count,
    viewed as viewed_count,
    purchased as purchased_count, 
    ${df_safe_cast_numeric('earnings', 'FLOAT64')} as earnings_total,
    message_text,
    source_file,
    CURRENT_TIMESTAMP() as loaded_at
  FROM messages_unioned
),

messages_final AS (
  SELECT
    *,
    DATE(sending_ts) as sending_date
  FROM messages_cleaned
  WHERE sending_ts IS NOT NULL
)

SELECT * FROM messages_final
WHERE sending_date >= DATE '2024-01-01'  -- Historical cutoff date

${ when(incremental(), `
  -- Incremental filter: Process recent data and catch late arrivals
  AND sending_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 14 DAY)
  AND (
    -- New records based on timestamp watermark
    sending_ts > (
      SELECT COALESCE(MAX(sending_ts), TIMESTAMP('2024-01-01'))
      FROM ${self()}
      WHERE sending_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)  -- Partition filter for efficiency
    )
    -- Or records that were recently loaded (catch late arrivals)
    OR loaded_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR)
  )
`) }

QUALIFY ROW_NUMBER() OVER (
  PARTITION BY message_sk 
  ORDER BY loaded_at DESC
) = 1